{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmHOWW0SoNXM"
   },
   "source": [
    "## 1. SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 279403,
     "status": "ok",
     "timestamp": 1758896227092,
     "user": {
      "displayName": "구자협 (산업정보시스템전공)",
      "userId": "09674769247970950690"
     },
     "user_tz": -540
    },
    "id": "SdkpgMl2-b_2",
    "outputId": "ac4c31bb-9a21-4257-b1a1-1f3ba4e9e8f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test shape: (1527298, 118)\n",
      "데이터 로드 완료\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# # zip_path = '/content/drive/MyDrive/2025_dacon_toss/data/toss_datasets.zip'\n",
    "# extract_path = './datas' # 압축 해제할 경로\n",
    "\n",
    "# test = pd.read_csv(r\"D:\\toss\\test_.csv\")\n",
    "# # print(f\"Train shape: {train.shape}\")\n",
    "# print(f\"Test shape: {test.shape}\")\n",
    "# print(\"데이터 로드 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5345,
     "status": "ok",
     "timestamp": 1758896441690,
     "user": {
      "displayName": "구자협 (산업정보시스템전공)",
      "userId": "09674769247970950690"
     },
     "user_tz": -540
    },
    "id": "Gm4qykhunvBB",
    "outputId": "625cabd9-6866-47c2-b1c0-44fe4ac3850e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "CFG = {\n",
    "    'BATCH_SIZE': 1024,\n",
    "    'EPOCHS': 5,\n",
    "    'LEARNING_RATE': 1e-3,\n",
    "    'SEED': 42\n",
    "}\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG['SEED'])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGK9aUXeoiR-"
   },
   "source": [
    "## 2. DATA LOADED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1758896441695,
     "user": {
      "displayName": "구자협 (산업정보시스템전공)",
      "userId": "09674769247970950690"
     },
     "user_tz": -540
    },
    "id": "uCd2bIHhoUyI",
    "outputId": "404d50a6-6ca3-4b52-9aa3-0304bf420566"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로드 시작\n",
      "Train shape: (612537, 119)\n",
      "Test shape: (1527298, 118)\n",
      "데이터 로드 완료\n",
      "Num features: 113 | Cat features: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"데이터 로드 시작\")\n",
    "train = pd.read_parquet(\"../data/train.parquet\", engine=\"pyarrow\")\n",
    "test = pd.read_parquet(\"../data/test.parquet\", engine=\"pyarrow\")\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(\"데이터 로드 완료\")\n",
    "\n",
    "target_col = \"clicked\"\n",
    "seq_col = \"seq\"\n",
    "FEATURE_EXCLUDE = {target_col, seq_col, \"ID\"}\n",
    "feature_cols = [c for c in train.columns if c not in FEATURE_EXCLUDE]\n",
    "\n",
    "cat_cols = [\"gender\", \"age_group\", \"inventory_id\", \"l_feat_14\"]\n",
    "num_cols = [c for c in feature_cols if c not in cat_cols]\n",
    "print(f\"Num features: {len(num_cols)} | Cat features: {len(cat_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TflAeV4UolRe"
   },
   "source": [
    "## 3. ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender unique categories: 3\n",
      "age_group unique categories: 9\n",
      "inventory_id unique categories: 18\n",
      "l_feat_14 unique categories: 3066\n"
     ]
    }
   ],
   "source": [
    "def encode_categoricals(train_df, test_df, cat_cols):\n",
    "    encoders = {}\n",
    "    for col in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        all_values = pd.concat([train_df[col], test_df[col]], axis=0).astype(str).fillna(\"UNK\")\n",
    "        le.fit(all_values)\n",
    "        train_df[col] = le.transform(train_df[col].astype(str).fillna(\"UNK\"))\n",
    "        test_df[col]  = le.transform(test_df[col].astype(str).fillna(\"UNK\"))\n",
    "        encoders[col] = le\n",
    "        print(f\"{col} unique categories: {len(le.classes_)}\")\n",
    "    return train_df, test_df, encoders\n",
    "\n",
    "train, test, cat_encoders = encode_categoricals(train, test, cat_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28133,
     "status": "ok",
     "timestamp": 1758896486025,
     "user": {
      "displayName": "구자협 (산업정보시스템전공)",
      "userId": "09674769247970950690"
     },
     "user_tz": -540
    },
    "id": "eSuF-onNok7h",
    "outputId": "ca740dbf-16fb-43c0-c572-40d53d443c55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender unique categories: 3\n",
      "age_group unique categories: 9\n",
      "inventory_id unique categories: 18\n",
      "l_feat_14 unique categories: 3286\n"
     ]
    }
   ],
   "source": [
    "def encode_categoricals(train_df, test_df, cat_cols):\n",
    "    encoders = {}\n",
    "    for col in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        all_values = pd.concat([train_df[col], test_df[col]], axis=0).astype(str).fillna(\"UNK\")\n",
    "        le.fit(all_values)\n",
    "        train_df[col] = le.transform(train_df[col].astype(str).fillna(\"UNK\"))\n",
    "        test_df[col]  = le.transform(test_df[col].astype(str).fillna(\"UNK\"))\n",
    "        encoders[col] = le\n",
    "        print(f\"{col} unique categories: {len(le.classes_)}\")\n",
    "    return train_df, test_df, encoders\n",
    "\n",
    "train, test, cat_encoders = encode_categoricals(train, test, cat_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXWj66Dqos80"
   },
   "source": [
    "## 4. MODULE DEFINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DMlFduBuotiw"
   },
   "outputs": [],
   "source": [
    "class ClickDataset(Dataset):\n",
    "    def __init__(self, df, num_cols, cat_cols, seq_col, target_col=None, has_target=True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.num_cols = num_cols\n",
    "        self.cat_cols = cat_cols\n",
    "        self.seq_col = seq_col\n",
    "        self.target_col = target_col\n",
    "        self.has_target = has_target\n",
    "        self.num_X = self.df[self.num_cols].astype(float).fillna(0).values\n",
    "        self.cat_X = self.df[self.cat_cols].astype(int).values\n",
    "        self.seq_strings = self.df[self.seq_col].astype(str).values\n",
    "        if self.has_target:\n",
    "            self.y = self.df[self.target_col].astype(np.float32).values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        num_x = torch.tensor(self.num_X[idx], dtype=torch.float)\n",
    "        cat_x = torch.tensor(self.cat_X[idx], dtype=torch.long)\n",
    "        s = self.seq_strings[idx]\n",
    "        if s:\n",
    "            arr = np.fromstring(s, sep=\",\", dtype=np.float32)\n",
    "        else:\n",
    "            arr = np.array([0.0], dtype=np.float32)\n",
    "        seq = torch.from_numpy(arr)\n",
    "        if self.has_target:\n",
    "            y = torch.tensor(self.y[idx], dtype=torch.float)\n",
    "            return num_x, cat_x, seq, y\n",
    "        else:\n",
    "            return num_x, cat_x, seq\n",
    "\n",
    "def collate_fn_train(batch):\n",
    "    num_x, cat_x, seqs, ys = zip(*batch)\n",
    "    num_x = torch.stack(num_x)\n",
    "    cat_x = torch.stack(cat_x)\n",
    "    ys = torch.stack(ys)\n",
    "    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)\n",
    "    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "    seq_lengths = torch.clamp(seq_lengths, min=1)\n",
    "    return num_x, cat_x, seqs_padded, seq_lengths, ys\n",
    "\n",
    "def collate_fn_infer(batch):\n",
    "    num_x, cat_x, seqs = zip(*batch)\n",
    "    num_x = torch.stack(num_x)\n",
    "    cat_x = torch.stack(cat_x)\n",
    "    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)\n",
    "    seq_lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "    seq_lengths = torch.clamp(seq_lengths, min=1)\n",
    "    return num_x, cat_x, seqs_padded, seq_lengths\n",
    "\n",
    "class CrossNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(input_dim, 1, bias=True) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x0):\n",
    "        x = x0\n",
    "        for w in self.layers:\n",
    "            x = x0 * w(x) + x\n",
    "        return x\n",
    "\n",
    "class WideDeepCTR(nn.Module):\n",
    "    def __init__(self, num_features, cat_cardinalities, emb_dim=16, lstm_hidden=64,\n",
    "                 hidden_units=[512,256,128], dropout=[0.1,0.2,0.3]):\n",
    "        super().__init__()\n",
    "        self.emb_layers = nn.ModuleList([\n",
    "            nn.Embedding(cardinality, emb_dim) for cardinality in cat_cardinalities\n",
    "        ])\n",
    "        cat_input_dim = emb_dim * len(cat_cardinalities)\n",
    "        self.bn_num = nn.BatchNorm1d(num_features)\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=lstm_hidden,\n",
    "                            num_layers=2, batch_first=True, bidirectional=True)\n",
    "        seq_out_dim = lstm_hidden * 2\n",
    "        self.cross = CrossNetwork(num_features + cat_input_dim + seq_out_dim, num_layers=2)\n",
    "        input_dim = num_features + cat_input_dim + seq_out_dim\n",
    "        layers = []\n",
    "        for i, h in enumerate(hidden_units):\n",
    "            layers += [nn.Linear(input_dim, h), nn.ReLU(), nn.Dropout(dropout[i % len(dropout)])]\n",
    "            input_dim = h\n",
    "        layers += [nn.Linear(input_dim, 1)]\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, num_x, cat_x, seqs, seq_lengths):\n",
    "        num_x = self.bn_num(num_x)\n",
    "        cat_embs = [emb(cat_x[:, i]) for i, emb in enumerate(self.emb_layers)]\n",
    "        cat_feat = torch.cat(cat_embs, dim=1)\n",
    "        seqs = seqs.unsqueeze(-1)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(seqs, seq_lengths.cpu(),\n",
    "                                                   batch_first=True, enforce_sorted=False)\n",
    "        _, (h_n, _) = self.lstm(packed)\n",
    "        h = torch.cat([h_n[-2], h_n[-1]], dim=1)\n",
    "        z = torch.cat([num_x, cat_feat, h], dim=1)\n",
    "        z_cross = self.cross(z)\n",
    "        out = self.mlp(z_cross)\n",
    "        return out.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kYtwWMrbEm5"
   },
   "source": [
    "### GDCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5cXUOUN8c6u9"
   },
   "outputs": [],
   "source": [
    "class GDCNModule(nn.Module):\n",
    "    def __init__(self, cat_cardinalities, emb_dim=16, cn_layers=3, mlp_layers=(400, 400, 400), dropout=0.5):\n",
    "        super().__init__()\n",
    "        # FeaturesEmbedding is assumed to be defined in models.basic_layers\n",
    "        self.embedding = FeaturesEmbedding(cat_cardinalities, emb_dim, concat=True)\n",
    "\n",
    "        if isinstance(emb_dim, int):\n",
    "            self.embed_output_dim = len(cat_cardinalities) * emb_dim\n",
    "        else:\n",
    "            self.embed_output_dim = sum(emb_dim)\n",
    "\n",
    "        self.cross_net = GateCorssLayer(self.embed_output_dim, cn_layers)\n",
    "        self.deep = MultiLayerPerceptron(self.embed_output_dim, mlp_layers, output_layer=False, dropout=dropout)\n",
    "        self.output_layer = nn.Linear(self.embed_output_dim + mlp_layers[-1], 1)\n",
    "\n",
    "    def forward(self, cat_x):\n",
    "        x_embed = self.embedding(cat_x)\n",
    "        cross_out = self.cross_net(x_embed)\n",
    "        deep_out = self.deep(x_embed)\n",
    "        out = self.output_layer(torch.cat([cross_out, deep_out], dim=1))\n",
    "        return out.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7tYSJ3FrbJmU"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from basic_layers import FeaturesEmbedding, MultiLayerPerceptron\n",
    "\n",
    "class GDCNP(nn.Module):\n",
    "    def __init__(self, field_dims, embed_dim, cn_layers=3, mlp_layers=(400, 400, 400), dropout=0.5):\n",
    "        super(GDCNP, self).__init__()\n",
    "        self.embedding = FeaturesEmbedding(field_dims, embed_dim, concat=True)\n",
    "        # self.embed_output_dim = len(field_dims) * embed_dim\n",
    "        if isinstance(embed_dim, int):\n",
    "            self.embed_output_dim = len(field_dims) * embed_dim\n",
    "        else:\n",
    "            self.embed_output_dim = sum(embed_dim)\n",
    "        self.cross_net = GateCorssLayer(self.embed_output_dim, cn_layers)\n",
    "        self.mlp = MultiLayerPerceptron(self.embed_output_dim, mlp_layers, output_layer=False, dropout=dropout)\n",
    "        self.fc = torch.nn.Linear(mlp_layers[-1] + self.embed_output_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_emb = self.embedding(x)\n",
    "        # x_emb = self.embedding(x).view(-1, self.embed_output_dim)\n",
    "        cross_cn = self.cross_net(x_emb)\n",
    "        cross_mlp = self.mlp(x_emb)\n",
    "        pred_y = self.fc(torch.cat([cross_cn, cross_mlp], dim=1))\n",
    "        return pred_y\n",
    "\n",
    "\n",
    "class GDCNS(torch.nn.Module):\n",
    "    def __init__(self, field_dims, embed_dim, cn_layers=3, mlp_layers=(400, 400, 400), dropout=0.5):\n",
    "        super(GDCNS, self).__init__()\n",
    "        self.embedding = FeaturesEmbedding(field_dims, embed_dim, concat=True)\n",
    "        if isinstance(embed_dim, int):\n",
    "            self.embed_output_dim = len(field_dims) * embed_dim\n",
    "        else:\n",
    "            self.embed_output_dim = sum(embed_dim)\n",
    "        self.cross_net = GateCorssLayer(self.embed_output_dim, cn_layers)\n",
    "        self.pred_layer = MultiLayerPerceptron(self.embed_output_dim, mlp_layers, output_layer=True,\n",
    "                                               dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_embed = self.embedding(x)\n",
    "        # x_embed = self.embedding(x).view(-1, self.embed_output_dim)\n",
    "        cross_cn = self.cross_net(x_embed)\n",
    "        pred_y = self.pred_layer(cross_cn)\n",
    "        return pred_y\n",
    "\n",
    "\n",
    "class GateCorssNetwork(torch.nn.Module):\n",
    "    def __init__(self, field_dims, embed_dim, cn_layers=3):\n",
    "        super(GateCorssNetwork, self).__init__()\n",
    "        self.embedding = FeaturesEmbedding(field_dims, embed_dim, concat=True)\n",
    "        if isinstance(embed_dim, int):\n",
    "            self.embed_output_dim = len(field_dims) * embed_dim\n",
    "        else:\n",
    "            self.embed_output_dim = sum(embed_dim)\n",
    "        self.cross_net = GateCorssLayer(self.embed_output_dim, cn_layers)\n",
    "        self.pred_layer = torch.nn.Linear(self.embed_output_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_embed = self.embedding(x)\n",
    "        cross_cn = self.cross_net(x_embed)\n",
    "        pred_y = self.pred_layer(cross_cn)\n",
    "        return pred_y\n",
    "\n",
    "\n",
    "class GateCorssLayer(nn.Module):\n",
    "    #  The core structure： gated corss layer.\n",
    "    def __init__(self, input_dim, cn_layers=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cn_layers = cn_layers\n",
    "\n",
    "        self.w = torch.nn.ModuleList([\n",
    "            torch.nn.Linear(input_dim, input_dim, bias=False) for _ in range(cn_layers)\n",
    "        ])\n",
    "        self.wg = torch.nn.ModuleList([\n",
    "            torch.nn.Linear(input_dim, input_dim, bias=False) for _ in range(cn_layers)\n",
    "        ])\n",
    "\n",
    "        self.b = torch.nn.ParameterList([torch.nn.Parameter(\n",
    "            torch.zeros((input_dim,))) for _ in range(cn_layers)])\n",
    "\n",
    "        for i in range(cn_layers):\n",
    "            torch.nn.init.uniform_(self.b[i].data)\n",
    "\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = x\n",
    "        for i in range(self.cn_layers):\n",
    "            xw = self.w[i](x) # Feature Crossing\n",
    "            xg = self.activation(self.wg[i](x)) # Information Gate\n",
    "            x = x0 * (xw + self.b[i]) * xg + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "a6BzdUfBZtw_"
   },
   "outputs": [],
   "source": [
    "class HybridGDCN(nn.Module):\n",
    "    def __init__(self, num_features, cat_cardinalities, emb_dim=16, lstm_hidden=64,\n",
    "                 cn_layers=3, mlp_layers=(400, 400, 400), dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = FeaturesEmbedding(cat_cardinalities, emb_dim, concat=True)\n",
    "        self.bn_num = nn.BatchNorm1d(num_features)\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=lstm_hidden, num_layers=2,\n",
    "                            batch_first=True, bidirectional=True)\n",
    "\n",
    "        if isinstance(emb_dim, int):\n",
    "            cat_feat_dim = len(cat_cardinalities) * emb_dim\n",
    "        else:\n",
    "            cat_feat_dim = sum(emb_dim)\n",
    "\n",
    "        self.seq_out_dim = lstm_hidden * 2\n",
    "        self.input_dim = cat_feat_dim + num_features + self.seq_out_dim\n",
    "\n",
    "        self.cross_net = GateCorssLayer(self.input_dim, cn_layers)\n",
    "        self.deep = MultiLayerPerceptron(self.input_dim, mlp_layers, output_layer=False, dropout=dropout)\n",
    "        self.output_layer = nn.Linear(self.input_dim + mlp_layers[-1], 1)\n",
    "\n",
    "    def forward(self, num_x, cat_x, seqs, seq_lengths):\n",
    "        num_x = self.bn_num(num_x)\n",
    "        cat_feat = self.embedding(cat_x)\n",
    "\n",
    "        seqs = seqs.unsqueeze(-1)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(seqs, seq_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (h_n, _) = self.lstm(packed)\n",
    "        h = torch.cat([h_n[-2], h_n[-1]], dim=1)\n",
    "\n",
    "        x = torch.cat([cat_feat, num_x, h], dim=1)\n",
    "        cross_out = self.cross_net(x)\n",
    "        deep_out = self.deep(x)\n",
    "        out = self.output_layer(torch.cat([cross_out, deep_out], dim=1))\n",
    "        return out.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1ECDSP_o8DV"
   },
   "source": [
    "## 5. TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "X6zktYB9cXIe"
   },
   "outputs": [],
   "source": [
    "def train_model(train_df, num_cols, cat_cols, seq_col, target_col, batch_size, epochs, lr, device, model_type='wide_deep'):\n",
    "    train_dataset = ClickDataset(train_df, num_cols, cat_cols, seq_col, target_col, True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                              collate_fn=collate_fn_train, pin_memory=True)\n",
    "    cat_cardinalities = [len(cat_encoders[c].classes_) for c in cat_cols]\n",
    "\n",
    "    if model_type == 'gdcn':\n",
    "        model = GDCNModule(cat_cardinalities, emb_dim=16).to(device)\n",
    "    elif model_type == 'hybrid_gdcn':\n",
    "        model = HybridGDCN(num_features=len(num_cols),\n",
    "                           cat_cardinalities=cat_cardinalities,\n",
    "                           emb_dim=16).to(device)\n",
    "    else:\n",
    "        model = WideDeepCTR(num_features=len(num_cols),\n",
    "                            cat_cardinalities=cat_cardinalities,\n",
    "                            emb_dim=16).to(device)\n",
    "\n",
    "    pos_weight_value = (len(train_df) - train_df[target_col].sum()) / train_df[target_col].sum()\n",
    "    pos_weight = torch.tensor([pos_weight_value], dtype=torch.float).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=2, T_mult=2)\n",
    "\n",
    "    print(\"학습 시작\")\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"[Train Epoch {epoch}]\"):\n",
    "            if model_type == 'gdcn':\n",
    "                num_x, cat_x, seqs, lens, ys = batch\n",
    "                cat_x, ys = cat_x.to(device), ys.to(device)\n",
    "                logits = model(cat_x)\n",
    "            else:\n",
    "                num_x, cat_x, seqs, lens, ys = batch\n",
    "                num_x, cat_x, seqs, lens, ys = num_x.to(device), cat_x.to(device), seqs.to(device), lens.to(device), ys.to(device)\n",
    "                logits = model(num_x, cat_x, seqs, lens)\n",
    "\n",
    "            loss = criterion(logits, ys)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item() * ys.size(0)\n",
    "\n",
    "\n",
    "        total_loss /= len(train_dataset)\n",
    "        print(f\"[Epoch {epoch}] Train Loss: {total_loss:.4f}\")\n",
    "    print(\"학습 완료\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6633645,
     "status": "ok",
     "timestamp": 1758904284261,
     "user": {
      "displayName": "구자협 (산업정보시스템전공)",
      "userId": "09674769247970950690"
     },
     "user_tz": -540
    },
    "id": "TkJ6qir0cd6S",
    "outputId": "8cfd789c-b282-4651-9e62-4f0870d22cae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 1]: 100%|██████████| 10454/10454 [21:59<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 1.2761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 2]: 100%|██████████| 10454/10454 [21:35<00:00,  8.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Loss: 1.2606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 3]: 100%|██████████| 10454/10454 [22:31<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Loss: 1.2549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 4]: 100%|██████████| 10454/10454 [22:09<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Loss: 1.2555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 5]: 100%|██████████| 10454/10454 [21:46<00:00,  8.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Loss: 1.2515\n",
      "학습 완료\n"
     ]
    }
   ],
   "source": [
    "# model = train_model(\n",
    "#     train_df=train,\n",
    "#     num_cols=num_cols,\n",
    "#     cat_cols=cat_cols,\n",
    "#     seq_col=seq_col,\n",
    "#     target_col=target_col,\n",
    "#     batch_size=CFG['BATCH_SIZE'],\n",
    "#     epochs=CFG['EPOCHS'],\n",
    "#     lr=CFG['LEARNING_RATE'],\n",
    "#     device=device,\n",
    "#     model_type='gdcn'  # Options: 'gdcn', 'hybrid_gdcn', 'wide_deep'\n",
    "# )\n",
    "\n",
    "# public lb : 0.321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25695939,
     "status": "ok",
     "timestamp": 1758930254441,
     "user": {
      "displayName": "구자협 (산업정보시스템전공)",
      "userId": "09674769247970950690"
     },
     "user_tz": -540
    },
    "id": "z9ksbWux4YPs",
    "outputId": "d3e60309-3ed3-4d43-d327-57b46686e70a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 1]: 100%|██████████| 10454/10454 [1:24:58<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 1.2576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 2]: 100%|██████████| 10454/10454 [1:25:50<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train Loss: 1.1859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 3]: 100%|██████████| 10454/10454 [1:25:38<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train Loss: 1.1633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 4]: 100%|██████████| 10454/10454 [1:25:22<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train Loss: 1.1666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Epoch 5]: 100%|██████████| 10454/10454 [1:26:03<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train Loss: 1.1504\n",
      "학습 완료\n"
     ]
    }
   ],
   "source": [
    "# model = train_model(\n",
    "#     train_df=train,\n",
    "#     num_cols=num_cols,\n",
    "#     cat_cols=cat_cols,\n",
    "#     seq_col=seq_col,\n",
    "#     target_col=target_col,\n",
    "#     batch_size=CFG['BATCH_SIZE'],\n",
    "#     epochs=CFG['EPOCHS'],\n",
    "#     lr=CFG['LEARNING_RATE'],\n",
    "#     device=device,\n",
    "#     model_type='hybrid_gdcn'  # Options: 'gdcn', 'hybrid_gdcn', 'wide_deep'\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1758930254573,
     "user": {
      "displayName": "구자협 (산업정보시스템전공)",
      "userId": "09674769247970950690"
     },
     "user_tz": -540
    },
    "id": "6d-En_JokROd",
    "outputId": "68b3ab0a-6768-4a91-f61f-852e17c7ab4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모델 저장 완료: 0926/model_hybrid_gdcn.pt\n"
     ]
    }
   ],
   "source": [
    "# os.makedirs('0926', exist_ok=True)\n",
    "# save_path = '0926/model_hybrid_gdcn.pt'\n",
    "# torch.save(model.state_dict(), save_path)\n",
    "# print(f\"✅ 모델 저장 완료: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8uty0qVo-2j"
   },
   "source": [
    "## 6. INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 385271,
     "status": "ok",
     "timestamp": 1758930639882,
     "user": {
      "displayName": "구자협 (산업정보시스템전공)",
      "userId": "09674769247970950690"
     },
     "user_tz": -540
    },
    "id": "KtbdB9wycpIZ",
    "outputId": "817c4acc-b0ed-4424-eee8-1d868aed32ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추론 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Inference]: 100%|██████████| 1492/1492 [06:19<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추론 완료\n",
      "제출 파일 저장 완료\n"
     ]
    }
   ],
   "source": [
    "# print(\"추론 시작\")\n",
    "# test_dataset = ClickDataset(test, num_cols, cat_cols, seq_col, has_target=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False,\n",
    "#                          collate_fn=collate_fn_infer, pin_memory=True)\n",
    "# model.eval()\n",
    "# outs = []\n",
    "# model_type = 'hybrid_gdcn'  # or 'hybrid_gdcn' or 'wide_deep'\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch in tqdm(test_loader, desc=\"[Inference]\"):\n",
    "#         if model_type == 'gdcn':  # GDCNP, GDCNS, GDCNModule\n",
    "#             num_x, cat_x, seqs, lens = batch\n",
    "#             cat_x = cat_x.to(device)\n",
    "#             pred = model(cat_x)\n",
    "#         else:  # wide_deep or hybrid_gdcn\n",
    "#             num_x, cat_x, seqs, lens = batch\n",
    "#             num_x, cat_x, seqs, lens = num_x.to(device), cat_x.to(device), seqs.to(device), lens.to(device)\n",
    "#             pred = model(num_x, cat_x, seqs, lens)\n",
    "\n",
    "#         outs.append(torch.sigmoid(pred).cpu())\n",
    "\n",
    "# test_preds = torch.cat(outs).numpy()\n",
    "# print(\"추론 완료\")\n",
    "\n",
    "# submit = pd.read_csv('datas/sample_submission.csv')\n",
    "# submit['clicked'] = test_preds\n",
    "# submit.to_csv('0926/hybrid_gdcn.csv', index=False)\n",
    "# print(\"제출 파일 저장 완료\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0d86sP9MJLC5"
   },
   "source": [
    "### 6.1 best model load infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZMQ6aTrqHw9U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing embedding.embed_dict.3.weight: torch.Size([3286, 16]) -> torch.Size([3066, 16])\n",
      "✅ Best 모델 로드 완료: ./model_hybrid_gdcn_5epch.pt\n",
      "추론 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Inference]: 100%|██████████| 1492/1492 [09:44<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추론 완료\n",
      "제출 파일 저장 완료: 1010/hybrid_gdcn_5epch_1_15.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "# model load\n",
    "best_model_path = \"./model_hybrid_gdcn_5epch.pt\"\n",
    "\n",
    "cat_cardinalities = [len(cat_encoders[c].classes_) for c in cat_cols]\n",
    "\n",
    "model = HybridGDCN(\n",
    "    num_features=len(num_cols),\n",
    "    cat_cardinalities=cat_cardinalities,\n",
    "    emb_dim=16,\n",
    "    lstm_hidden=64,\n",
    "    cn_layers=3,\n",
    "    mlp_layers=(400, 400, 400),\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "state_dict = torch.load(best_model_path, map_location=\"cpu\")\n",
    "model_state = model.state_dict()\n",
    "\n",
    "for k, v in state_dict.items():\n",
    "    if k in model_state and v.shape != model_state[k].shape:\n",
    "        old_shape, new_shape = v.shape, model_state[k].shape\n",
    "        print(f\"Resizing {k}: {old_shape} -> {new_shape}\")\n",
    "\n",
    "        # 새 weight로 복사\n",
    "        new_v = model_state[k]\n",
    "        copy_len = min(old_shape[0], new_shape[0])\n",
    "        new_v[:copy_len] = v[:copy_len]\n",
    "        state_dict[k] = new_v\n",
    "\n",
    "# 로드\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model.eval()\n",
    "print(f\"✅ Best 모델 로드 완료: {best_model_path}\")\n",
    "\n",
    "print(\"추론 시작\")\n",
    "test_dataset = ClickDataset(test, num_cols, cat_cols, seq_col, has_target=False)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CFG['BATCH_SIZE'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn_infer,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "outs = []\n",
    "model_type = 'hybrid_gdcn'  # or 'gdcn' or 'wide_deep'\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"[Inference]\"):\n",
    "        if model_type == 'gdcn':  # GDCNP, GDCNS, GDCNModule\n",
    "            num_x, cat_x, seqs, lens = batch\n",
    "            cat_x = cat_x.to(device)\n",
    "            pred = model(cat_x)\n",
    "        else:  # wide_deep or hybrid_gdcn\n",
    "            num_x, cat_x, seqs, lens = batch\n",
    "            num_x, cat_x, seqs, lens = (\n",
    "                num_x.to(device),\n",
    "                cat_x.to(device),\n",
    "                seqs.to(device),\n",
    "                lens.to(device),\n",
    "            )\n",
    "            pred = model(num_x, cat_x, seqs, lens)\n",
    "\n",
    "        outs.append(torch.sigmoid(pred).cpu())\n",
    "\n",
    "test_preds = torch.cat(outs).numpy()\n",
    "print(\"추론 완료\")\n",
    "\n",
    "# 제출 파일 저장\n",
    "os.makedirs(\"1010\", exist_ok=True)\n",
    "submit = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "submit[\"clicked\"] = test_preds\n",
    "submit_path = \"hybrid_gdcn_5epch_1_15.csv\"\n",
    "submit.to_csv(submit_path, index=False)\n",
    "print(f\"제출 파일 저장 완료: {submit_path}\")\n",
    "# lb 0.34686\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 0.9999999999999827\n",
      "Mean: 0.3641048705529084\n",
      "Median: 0.3452497093339333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def apply_temperature(p, T=1.01):\n",
    "    eps = 1e-15\n",
    "    p = np.clip(p, eps, 1-eps)             # 확률 안정화\n",
    "    logit = np.log(p / (1 - p))            # logit 변환\n",
    "    p_T = 1 / (1 + np.exp(-logit / T))     # scaling\n",
    "    return p_T\n",
    "\n",
    "sub = pd.read_csv(\"hybrid_gdcn_5epch_1_15.csv\")\n",
    "\n",
    "sub[\"clicked\"] = apply_temperature(sub[\"clicked\"], T=1.09)\n",
    "\n",
    "print(\"Max:\", sub[\"clicked\"].max())\n",
    "print(\"Mean:\", sub[\"clicked\"].mean())\n",
    "print(\"Median:\", sub[\"clicked\"].median())\n",
    "sub.to_csv(\"../output/hybrid_gdcn.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54wyypNCJP1u"
   },
   "source": [
    "### 6.2 csv 후처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkddCEundFCq"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # 1. csv 파일 불러오기\n",
    "# df = pd.read_csv('hybrid_gdcn_5epch_1_15.csv')\n",
    "\n",
    "# # 2. 확률값 → logit(시그모이드 역함수)\n",
    "# def prob_to_logit(p):\n",
    "#     eps = 1e-7\n",
    "#     p = np.clip(p, eps, 1-eps)\n",
    "#     return np.log(p / (1 - p))\n",
    "\n",
    "# logits = prob_to_logit(df['clicked'].values)\n",
    "\n",
    "# # 3. logit 값 조정 (클리핑, 노이즈, 스케일링, temperature scaling)\n",
    "# # logits = np.clip(logits, -0.1, 0.1)\n",
    "# # logits = logits + np.random.normal(0, 0.5, size=logits.shape)\n",
    "# # logits = logits * 0.8\n",
    "\n",
    "# # Temperature scaling 적용 (T > 1이면 확률 완만)\n",
    "# temperature = 1.09\n",
    "# logits = logits / temperature\n",
    "\n",
    "# # 4. 다시 확률로 변환\n",
    "# def logit_to_prob(l):\n",
    "#     return 1 / (1 + np.exp(-l))\n",
    "\n",
    "# df['clicked'] = logit_to_prob(logits)\n",
    "\n",
    "# # 5. 새로운 csv로 저장\n",
    "# df.to_csv('1010/hybrid_gdcn_5epch_1_15_temp_1_09.csv', index=False)\n",
    "# print('조정된 확률값으로 저장 완료!')\n",
    "\n",
    "# # lb 34699"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPatj8qEuuEg5pLvCX6i0dy",
   "gpuType": "A100",
   "mount_file_id": "18Okrwt-AVTpfyrrv575ps3RLpiuUneIW",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ocr_last",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
